# awesome-models


[![Awesome](_static/awesome.png)](https://github.com/partheee/awesome-models)

The Awesome Models repository is a collection of links to pre-trained models.If you find any broken links, please raise an issue to have them fixed as soon as possible.
## 🚀 PyTorch - Image classification
| Framework                         | Model_link 	|
|------------                      	|------------	|
|AlexNet|[model link](https://download.pytorch.org/models/alexnet-owt-7be5be79.pth)|
|ConvNeXt_Tiny|[model link](https://download.pytorch.org/models/convnext_tiny-983f1562.pth)|
|ConvNeXt_Small|[model link](https://download.pytorch.org/models/convnext_small-0c510722.pth)|
|ConvNeXt_Base|[model link](https://download.pytorch.org/models/convnext_base-6075fbad.pth)|
|ConvNeXt_Large|[model link](https://download.pytorch.org/models/convnext_large-ea097f82.pth)|
|DenseNet_121|[model link](https://download.pytorch.org/models/densenet121-a639ec97.pth)|
|DenseNet_161|[model link](https://download.pytorch.org/models/densenet161-8d451a50.pth)|
|DenseNet_169|[model link](https://download.pytorch.org/models/densenet169-b2777c0a.pth)|
|DenseNet_201|[model link](https://download.pytorch.org/models/densenet201-c1103571.pth)|
|EfficientNet_B0|[model link](https://download.pytorch.org/models/efficientnet_b0_rwightman-3dd342df.pth)|
|EfficientNet_B1|[model link](https://download.pytorch.org/models/efficientnet_b1_rwightman-533bc792.pth)|
|EfficientNet_B2|[model link](https://download.pytorch.org/models/efficientnet_b2_rwightman-bcdf34b7.pth)|
|EfficientNet_B3|[model link](https://download.pytorch.org/models/efficientnet_b3_rwightman-cf984f9c.pth)|
|EfficientNet_B4|[model link](https://download.pytorch.org/models/efficientnet_b4_rwightman-7eb33cd5.pth)|
|EfficientNet_B5|[model link](https://download.pytorch.org/models/efficientnet_b5_lukemelas-b6417697.pth)|
|EfficientNet_B6|[model link](https://download.pytorch.org/models/efficientnet_b6_lukemelas-c76e70fd.pth)|
|EfficientNet_B7|[model link](https://download.pytorch.org/models/efficientnet_b7_lukemelas-dcc49843.pth)|
|EfficientNet_V2_S|[model link](https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth)|
|EfficientNet_V2_M|[model link](https://download.pytorch.org/models/efficientnet_v2_m-dc08266a.pth)|
|EfficientNet_V2_L|[model link](https://download.pytorch.org/models/efficientnet_v2_l-59c71312.pth)|
|GoogLeNet|[model link](https://download.pytorch.org/models/googlenet-1378be20.pth)|
|Inception V3|[model link](https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth)|
|MaxVit|[model link](https://download.pytorch.org/models/maxvit_t-bc5ab103.pth)|
|MNASNet0_5|[model link](https://download.pytorch.org/models/mnasnet0.5_top1_67.823-3ffadce67e.pth)|
|MNASNet0_75|[model link](https://download.pytorch.org/models/mnasnet0_75-7090bc5f.pth)|
|MNASNet1_0|[model link](https://download.pytorch.org/models/mnasnet1.0_top1_73.512-f206786ef8.pth)|
|MNASNet1_3|[model link](https://download.pytorch.org/models/mnasnet1_3-a4c69d6f.pth)|
|MobileNet V2|[model link](https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth)|
|MobileNet V3_Small|[model link](https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth)|
|MobileNet V3_Large|[model link](https://download.pytorch.org/models/mobilenet_v3_large-5c1a4163.pth)|
|RegNet_Y_400mf|[model link](https://download.pytorch.org/models/regnet_y_400mf-e6988f5f.pth)|
|RegNet_Y_800mf|[model link](https://download.pytorch.org/models/regnet_y_800mf-58fc7688.pth)|
|RegNet_Y_1_6gf|[model link](https://download.pytorch.org/models/regnet_y_1_6gf-0d7bc02a.pth)|
|RegNet_Y_3_2gf|[model link](https://download.pytorch.org/models/regnet_y_3_2gf-9180c971.pth)|
|RegNet_Y_8gf|[model link](https://download.pytorch.org/models/regnet_y_8gf-d0d0e4a8.pth)|
|RegNet_Y_16gf|[model link](https://download.pytorch.org/models/regnet_y_1_6gf-0d7bc02a.pth)|
|RegNet_Y_32gf|[model link](https://download.pytorch.org/models/regnet_y_3_2gf-9180c971.pth)|
|RegNet_Y_128gf|[model link](https://download.pytorch.org/models/regnet_y_128gf_swag-c8ce3e52.pth)|
|RegNet_X_400mf|[model link](https://download.pytorch.org/models/regnet_x_400mf-62229a5f.pth)|
|RegNet_X_800mf|[model link](https://download.pytorch.org/models/regnet_x_800mf-94a99ebd.pth)|
|RegNet_X_1_6gf|[model link](https://download.pytorch.org/models/regnet_x_1_6gf-a12f2b72.pth)|
|RegNet_X_3_2gf|[model link](https://download.pytorch.org/models/regnet_x_3_2gf-7071aa85.pth)|
|RegNet_X_8gf|[model link](https://download.pytorch.org/models/regnet_x_8gf-2b70d774.pth)|
|RegNet_X_16gf|[model link](https://download.pytorch.org/models/regnet_x_16gf-ba3796d7.pth)|
|RegNet_X_32gf|[model link](https://download.pytorch.org/models/regnet_x_32gf-6eb8fdc6.pth)|
|ResNet_18|[model link](https://download.pytorch.org/models/resnet18-f37072fd.pth)|
|ResNet_34|[model link](https://download.pytorch.org/models/resnet34-b627a593.pth)|
|ResNet_50|[model link](https://download.pytorch.org/models/resnet50-11ad3fa6.pth)|
|ResNet_101|[model link](https://download.pytorch.org/models/resnet101-cd907fc2.pth)|
|ResNet_152|[model link](https://download.pytorch.org/models/resnet152-f82ba261.pth)|
|ResNeXt50_32x4d|[model link](https://download.pytorch.org/models/resnext50_32x4d-1a0047aa.pth)|
|ResNeXt101_32x8d|[model link](https://download.pytorch.org/models/resnext101_32x8d-110c445d.pth)|
|ResNeXt101_64x4d|[model link](https://download.pytorch.org/models/resnext101_64x4d-173b62eb.pth)|
|ShuffleNetV2_x0_5|[model link](https://download.pytorch.org/models/shufflenetv2_x0.5-f707e7126e.pth)|
|ShuffleNetV2_x1_0|[model link](https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth)|
|ShuffleNetV2_x1_5|[model link](https://download.pytorch.org/models/shufflenetv2_x1_5-3c479a10.pth)|
|ShuffleNetV2_x2_0|[model link](https://download.pytorch.org/models/shufflenetv2_x2_0-8be3c8ee.pth)|
|SqueezeNet1_0|[model link](https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth)|
|SqueezeNet1_1|[model link](https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth)|
|SwinTransformer_t|[model link](https://download.pytorch.org/models/swin_t-704ceda3.pth)|
|SwinTransformer_s|[model link](https://download.pytorch.org/models/swin_s-5e29d889.pth)|
|SwinTransformer_b|[model link](https://download.pytorch.org/models/swin_b-68c6b09e.pth)|
|SwinTransformer_v2_t|[model link](https://download.pytorch.org/models/swin_v2_t-b137f0e2.pth)|
|SwinTransformer_v2_s|[model link](https://download.pytorch.org/models/swin_v2_s-637d8ceb.pth)|
|SwinTransformer_v2_b|[model link](https://download.pytorch.org/models/swin_v2_b-781e5279.pth)|
|VGG11|[model link](https://download.pytorch.org/models/vgg11-8a719046.pth)|
|VGG11_bn|[model link](https://download.pytorch.org/models/vgg11_bn-6002323d.pth)|
|VGG13|[model link](https://download.pytorch.org/models/vgg13-19584684.pth)|
|VGG13_bn|[model link](https://download.pytorch.org/models/vgg13_bn-abd245e5.pth)|
|VGG16|[model link](https://download.pytorch.org/models/vgg16-397923af.pth)|
|VGG16_bn|[model link](https://download.pytorch.org/models/vgg16_bn-6c64b313.pth)|
|VGG19|[model link](https://download.pytorch.org/models/vgg19-dcbb9e9d.pth)|
|VGG19_bn|[model link](https://download.pytorch.org/models/vgg19_bn-c79401a0.pth)|
|VisionTransformer_b_16|[model link](https://download.pytorch.org/models/vit_b_16-c867db91.pth)|
|VisionTransformer_b_32|[model link](https://download.pytorch.org/models/vit_b_32-d86f8d99.pth)|
|VisionTransformer_l_16|[model link](https://download.pytorch.org/models/vit_l_16-852ce7e3.pth)|
|VisionTransformer_l_32|[model link](https://download.pytorch.org/models/vit_l_32-c7638314.pth)|
|VisionTransformer_h_14|[model link](https://download.pytorch.org/models/vit_h_14_swag-80465313.pth)|
|Wide ResNet50_2|[model link](https://download.pytorch.org/models/wide_resnet50_2-9ba9bcbe.pth)|
|Wide ResNet101_2|[model link](https://download.pytorch.org/models/wide_resnet101_2-d733dc28.pth)|




## 🚀 PyTorch - Object Detection
| Framework                         | Model_link 	|
|------------                      	|------------	|
|Faster R-CNN_resnet50_fpn|[model link](https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth)|
|Faster R-CNN_resnet50_fpn_V2|[model link](https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth)|
|Faster R-CNN_mobilenet_v3_large_fpn|[model link](https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth)|
|Faster R-CNN_mobilenet_v3_large_320_fpn|[model link](https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_320_fpn-907ea3f9.pth)|
|FCOS_resnet50_fpn|[model link](https://download.pytorch.org/models/fcos_resnet50_fpn_coco-99b0c9b7.pth)|
|RetinaNet_resnet50_fpn|[model link](https://download.pytorch.org/models/retinanet_resnet50_fpn_coco-eeacb38b.pth)|
|RetinaNet_resnet50_fpn_v2|[model link](https://download.pytorch.org/models/retinanet_resnet50_fpn_v2_coco-5905b1c5.pth)|
|SSD300_vgg16|[model link](https://download.pytorch.org/models/ssd300_vgg16_coco-b556d3b4.pth)|
|SSDlite320_mobilenet_v3_large|[model link](https://download.pytorch.org/models/ssd300_vgg16_coco-b556d3b4.pth)|
## 🚀 Keras
| Framework                         | Model_link 	|
|------------                      	|------------	|
|Xception|[model link](https://github.com/modelhub-ai/xception/releases/download/v1.0/model.h5)|
|VGG16|[model link](https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5)|
|VGG19|[model link](https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5)|
|ResNet50|[model link](https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5)|
|ResNet50V2|[model link](https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5)|
|ResNet101|[model link](https://github.com/keras-team/keras-applications/releases/download/resnet/resnet101_weights_tf_dim_ordering_tf_kernels.h5)|
|ResNet101V2|[model link](https://github.com/keras-team/keras-applications/releases/download/resnet/resnet101v2_weights_tf_dim_ordering_tf_kernels.h5)|
|ResNet152|[model link](https://github.com/keras-team/keras-applications/releases/download/resnet/resnet152_weights_tf_dim_ordering_tf_kernels.h5)|
|ResNet152V2|[model link](https://github.com/keras-team/keras-applications/releases/download/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5)|
|InceptionV3|[model link](https://github.com/fchollet/deep-learning-models/releases/download/v0.2/inception_v3_weights_tf_dim_ordering_tf_kernels.h5)|
|InceptionResNetV2|[model link](https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5)|
|MobileNet|[model link](https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_128_tf.h5)|
|MobileNetV2|[model link](https://github.com/titu1994/keras-one-cycle/blob/master/models/mobilenet/weights/mobilenet_v2.h5)|
|DenseNet121|[model link](https://github.com/keras-team/keras-applications/releases/download/densenet/densenet121_weights_tf_dim_ordering_tf_kernels.h5)|
|DenseNet169|[model link](https://github.com/keras-team/keras-applications/releases/download/densenet/densenet169_weights_tf_dim_ordering_tf_kernels.h5)|
|DenseNet201|[model link](https://github.com/keras-team/keras-applications/releases/download/densenet/densenet201_weights_tf_dim_ordering_tf_kernels.h5)|
|NASNetMobile|[model link](https://github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-mobile.h5)|
|NASNetLarge|[model link](https://github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-large.h5)|
|EfficientNetB0|[model link](https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000.h5)|
|EfficientNetB1|[model link](https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b1_imagenet_1000.h5)|
|EfficientNetB2|[model link](https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b2_imagenet_1000.h5)|
|EfficientNetB3|[model link](https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b3_imagenet_1000.h5)|
|EfficientNetB4|[model link](https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b4_imagenet_1000_notop.h5)|
|EfficientNetB5|[model link](https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b5_imagenet_1000.h5)|
|EfficientNetB6|[model link](https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b6_noisy-student.h5)|
|EfficientNetB7|[model link](https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b7_noisy-student.h5)|
|EfficientNetV2B0|[model link](https://github.com/sebastian-sz/efficientnet-v2-keras/releases/download/v2.0/efficientnetv2-b0.h5)|
|EfficientNetV2B1|[model link](https://github.com/sebastian-sz/efficientnet-v2-keras/releases/download/v2.0/efficientnetv2-b1.h5)|
|EfficientNetV2B2|[model link](https://github.com/sebastian-sz/efficientnet-v2-keras/releases/download/v2.0/efficientnetv2-b2.h5)|
|EfficientNetV2B3|[model link](https://github.com/sebastian-sz/efficientnet-v2-keras/releases/download/v2.0/efficientnetv2-b3.h5)|
|EfficientNetV2S|[model link](https://github.com/sebastian-sz/efficientnet-v2-keras/releases/download/v2.0/efficientnetv2-s.h5)|
|EfficientNetV2M|[model link](https://github.com/sebastian-sz/efficientnet-v2-keras/releases/download/v2.0/efficientnetv2-m.h5)|
|EfficientNetV2L|[model link](https://github.com/sebastian-sz/efficientnet-v2-keras/releases/download/v2.0/efficientnetv2-l.h5)|
|ConvNeXtTiny|[model link](https://github.com/leondgarse/keras_cv_attention_models/releases/download/convnext/convnext_tiny_imagenet.h5)|
|ConvNeXtSmall|[model link](https://github.com/leondgarse/keras_cv_attention_models/releases/download/convnext/convnext_small_imagenet.h5)|
|ConvNeXtBase|[model link](https://github.com/leondgarse/keras_cv_attention_models/releases/download/convnext/convnext_base_384_imagenet.h5)|
|ConvNeXtLarge|[model link](https://github.com/leondgarse/keras_cv_attention_models/releases/download/convnext/convnext_large_384_imagenet.h5)|
|ConvNeXtXLarge|[model link](https://github.com/leondgarse/keras_cv_attention_models/releases/download/convnext/convnext_xlarge_384_imagenet21k-ft1k.h5)|
|Mask R-CNN|[model link](https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5)|
|AlexNet|[model link](https://www.lamsade.dauphine.fr/~bnegrevergne/rasta_kdd/python/models/weights/alexnet_weights.h5)|


## 🚀 Tensorflow
| Framework                         | Model_link 	|
|------------                      	|------------	|
|Efficientnetv2-S|[model link](https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2)|
|Efficientnetv2-M|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2)|
|Efficientnetv2-L|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2)|
|Efficientnetv2-S-21K|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2)|
|Efficientnetv2-M-21K|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2)|
|Efficientnetv2-L-21K|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2)|
|Efficientnetv2-Xl-21K|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2)|
|Efficientnetv2-B0-21K|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2,)|
|Efficientnetv2-B1-21K|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2)|
|Efficientnetv2-B2-21K|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2)|
|Efficientnetv2-B3-21K|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2)|
|Efficientnetv2-S-21K-ft1K|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2)|
|Efficientnetv2-M-21K-ft1K|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2)|
|Efficientnetv2-L-21K-ft1K|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2)|
|Efficientnetv2-Xl-21K-ft1K|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2)|
|Efficientnetv2-B0-21K-ft1K|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2)|
|Efficientnetv2-B1-21K-ft1K|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2)|
|Efficientnetv2-B2-21K-ft1K|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2)|
|Efficientnetv2-B3-21K-ft1K|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2)|
|Efficientnetv2-B0|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2)|
|Efficientnetv2-B1|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2)|
|Efficientnetv2-B2|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2)|
|Efficientnetv2-B3|[model link]( https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2)|
|Efficientnet_B0|[model link]( https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1)|
|Efficientnet_B1|[model link]( https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1)|
|Efficientnet_B2|[model link]( https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1)|
|Efficientnet_B3|[model link]( https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1)|
|Efficientnet_B4|[model link]( https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1)|
|Efficientnet_B5|[model link]( https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1)|
|Efficientnet_B6|[model link]( https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1)|
|Efficientnet_B7|[model link]( https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1)|
|Bit_S-R50X1|[model link]( https://tfhub.dev/google/bit/s-r50x1/1)|
|Inception_V3|[model link]( https://tfhub.dev/google/tf2-preview/inception_v3/classification/4)|
|Inception_Resnet_V2|[model link]( https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4)|
|Resnet_V1_50|[model link]( https://tfhub.dev/google/imagenet/inception_resnet_v2/classification/5)|
|Resnet_V1_101|[model link]( https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4)|
|Resnet_V1_152|[model link]( https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4)|
|Resnet_V2_50|[model link]( https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4)|
|Resnet_V2_101|[model link]( https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4)|
|Resnet_V2_152|[model link]( https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4)|
|Nasnet_Large|[model link]( https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4)|
|Nasnet_Mobile|[model link]( https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4)|
|Pnasnet_Large|[model link]( https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4)|
|Mobilenet_V2_100_224|[model link]( https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4)|
|Mobilenet_V2_130_224|[model link]( https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4)|
|Mobilenet_V2_140_224|[model link]( https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4)|
|Mobilenet_V3_Small_100_224|[model link]( https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5)|
|Mobilenet_V3_Small_075_224|[model link]( https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5)|
|Mobilenet_V3_Large_100_224|[model link]( https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5)|
|Mobilenet_V3_Large_075_224|[model link]( https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5)|






## 🐞 bugs & 🦸 contribution
Computer Vision moves fast! Sometimes our model links are invalid. If you notice that any of the model link is not working properly, create a bug report and let us know.

If you have an idea for a new model we should do, create a feature request. We are constantly looking for new models.

We are here for you, so don't hesitate to [reach out](https://www.linkedin.com/in/parthibanma/)